{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS||\n",
    "---------||\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0206| CEF, updated and spell checked. \n",
    "2018-0208| CEF, minor text update.\n",
    "2018-0305| CEF, updated with SHN comments.\n",
    "2019-0902| CEF, updated for ITMAL v2.\n",
    "2019-0904| CEF, updated and added conclusion Q.\n",
    "\n",
    "## Implementing a dummy classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader from Scikit-learn. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "#### Qb  Load and display the MNIST data\n",
    "\n",
    "There is a `sklearn.datasets.fetch_openml` dataloader interface in Scikit-learn. You can load MNIST data like \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',??) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert at scale (not always needed)\n",
    "#X = X / 255.\n",
    "```\n",
    "\n",
    "but you need to set parameters like `return_X_y` and `cache` if the default values are not suitable! \n",
    "\n",
    "Check out the documentation for the `fetch_openml` MNIST loader, try it out by loading a (X,y) MNIST data set, and plot a single digit via the `MNIST_PlotDigit` function here (input data is a 28x28 NMIST subimage)\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "```\n",
    "\n",
    "Finally, put the MNIST loader into a single function called `MNIST_GetDataSet()` so you can resuse it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa Answers\n",
    "\n",
    "When using the MNIST dataset we must use the function fetch_openml. We use the parameter return_X_y to get X matrix and y vector returned separately. X is a 70000 x 784  matrix  where the 784 define the images 28 x 28 pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa Answers\n",
    "\n",
    "When using the MNIST dataset we must use the function fetch_openml. We use the parameter return_X_y to get X matrix and y vector returned separately. X is a 70000 x 784  matrix  where the 784 define the images 28 x 28 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', cache=True, return_X_y=True) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert at scale (not always needed)\n",
    "#X = X / 255.\n",
    "\n",
    "# define MNIST_GetDataSet function:\n",
    "def MNIST_GetDataSet():\n",
    "    return fetch_openml('mnist_784',cache=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define MNIST_PlotDigit function:\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a single digit:\n",
    "MNIST_PlotDigit(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOLM], p82.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.(We will be looking at cross-validation instead of the simple fit-predict in a later exercise.)\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader...\n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Answers\n",
    "\n",
    "The code below follows the approach of running a SGD Classifier from [HOLM].\n",
    "\n",
    "Firstly, the data is split in a traning and test set in order to test and evaluate the algorithm on data it has not been trained on, this is very important.\n",
    "\n",
    "The y vector is then redefined in terms of whether the true result of all the images is '5'. This so that the problem can be approach as binary classication.\n",
    "\n",
    "After all the setup a the scikit learn fit-predict interface is used on the SGDClassifier; a  object of this class is instantiated. Then this object's fit method is called passing in the test set both X_train and y_train_5 (the training data and the corresponding true values), traing the classifier. The last step is to test the classifier on \"new\" (test) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test data (already loaded):\n",
    "X_train,X_test,y_train,y_test=X[:60000],X[60000:],y[:60000],y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare y vector to '5'\n",
    "y_train_5 = (y_train == '5')\n",
    "y_test_5  = (y_test == '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SDGClassifier object\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit training data\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predict using test data\n",
    "MNIST_PlotDigit(X_test[-2])\n",
    "sgd_clf.predict([X_test[-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Answers continued\n",
    "\n",
    "Below the are some score metrics applied to the SGDClassifier to evalute it's predictions. From the cross validation function the accuracy of the classifier is shown to be above 95 percent. But the accuracy alone can be misleading as a measure of the quality of our algorithm, specially when the data is skewed as it is in this dataset. The number of images in the classified as a '5' is far smaller than those classified as \"not '5'\". A high accuracy could be achieved by just \"guessing\" that the images does not contain a '5'. This is seen in the next exercise implementing the \"Dummy Classifier\". A such algorithm can't really be used to make meaningful prediction despite it having a high accuracy score.\n",
    "\n",
    "Therefore the confusion mattix for the SGDClassifier is also calculated. This shows that it predicted 52316 \"True Negatives\", 601 \"False Negatives\", 2263 \"False Positives\" and 4820 \"True Positives\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95035, 0.96035, 0.9604 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use cross-validation to score the classifier:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf,X_train,y_train_5,cv=3,scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict \n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53892,   687],\n",
       "       [ 1891,  3530]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc Implement a dummy binary classifier\n",
    "\n",
    "Now we will try to create a Scikit-learn compatible estimator implemented via a python class. Follow the code found in [HOML], p84, but name you estimator `DummyClassifier` instead of `Never5Classifyer`.\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L02/Figs/class_base_estimator.png\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit from `BaseEstimator` (and possibly also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duck typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and find a way to extract the accuracy `score` from the test data. You may implement an accuracy function yourself or just use the `sklearn.metrics.accuracy_score` function. \n",
    "\n",
    "Finally, compare the accuracy score from your `DummyClassifier` with the scores found in [HOML] \"Measuring Accuracy Using Cross-Validation\", p.83. Are they comparable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Anwsers\n",
    "\n",
    "Below is the DummyClassifier defined, returning a vector only containing False for a number of entries equal to the length of X, the number of images.\n",
    "\n",
    "Similarly to the SGDClassifier the DummyClassisier has been scored using the same functions. Even though the DummyClassifier has an accuracy of 90 percent, it performs no \"intelligent\" operations or predictions. This is due to the skewed dataset as mentioned earlier\n",
    "\n",
    "The confusion matrix of the DummyClassifier consists of: 54579 TN, 5421 FN, 0 TP and 0 FP. Compared to that of the SGDClassifier, the DummyClassifier correctly performs better when naming \"true negative\" results and there are no \"false positives\". But it has a higher number of \"false negatives\" and none \"true positives\". This of course fits with the definition of class.\n",
    "Even though the accuracy of the two classes are close, the DummyClassifier performs worse overall, as it can not give any meaningful predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class DummyClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self,X,y=None):\n",
    "        pass\n",
    "    def predict(self,X):\n",
    "        return np.zeros((len(X),1),dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90965, 0.90965, 0.90965])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmmy_clf = DummyClassifier()\n",
    "cross_val_score(dmmy_clf,X_train,y_train_5,cv=3,scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54579,     0],\n",
       "       [ 5421,     0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = cross_val_predict(dmmy_clf, X_train, y_train_5, cv=3)\n",
    "confusion_matrix(y_train_5,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Answers\n",
    "\n",
    "This exercise has been an great introduction to a second application of ML, classification. It has also been benifitial as an introdction to the using the scikit fit-predict interface, and the process that needs to be followed when working on a problem using ML, overall. These are:\n",
    "- Aquiring a dataset (and labelling the set), which has been done for us\n",
    "- Splitting the data into into a training and test set, as not to score the algorithm using the set it has been fitted on.\n",
    "- Performing the fit using the trainging data and correct labels.\n",
    "- Testing and scoring the the algorithm ona new set. \n",
    "\n",
    "The exercise has also shown the downside of relying to much on the accuracy score on its own, and that you have to consider the specifics and shape of your dataset when performing this score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
