{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supergruppe diskussion\n",
    "\n",
    "## § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel (eksklusiv\"Create the Workspace\" og \"Download the Data\"), og forbered mundtlig præsentation.\n",
    "\n",
    "Lav et kort resume af de enkelte underafsnit, ca. 5 til 20 liners tekst.\n",
    "\n",
    "Husk at relater til \"The Map\":\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L07/Figs/ml_supervised_map.png\" style=\"width:400px\">\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* Look at the Big Picture\n",
    "* Get the Data (eksklusiv Create the Workspace og Download the Data),\n",
    "* Discover and Visualize the Data to Gain Insights,\n",
    "* Prepare the Data for Machine Learning Algorithms,\n",
    "* Select and Train a Model,\n",
    "* Fine-Tune Your Model,\n",
    "* Launch, Monitor, and Maintain Your System,\n",
    "* Try It Out!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Look at the Big Picture\n",
    "\n",
    "- What is the problem - e.g What exactly needs to be solved. Example from the book - aside from housing prices based on their locatio)(In the specific case it will be used as a part of a pipeline).\n",
    "- Do we have points of comparison, precision, costs, etc..\n",
    "- How do we solve this problem, what type it: Supervised, unsupervised, regression or classification?\n",
    "    - Book: Supervised: labelled dataset, Regression: Finding relation between location and pricing\n",
    "    - Own: Classification of genre based on other features defined on music.\n",
    "- Choose a performance measure (cost function). What excatly does it tell us?\n",
    "    - Known algoritms RMSE(l2), MSE(l1). standard vector distances(l2 = norm). \n",
    "- Remember to reflect on above - more choices. What us the purpose, it is not necessary to work to develop a accurate regression model if housing prices if the output is used for classification later.\n",
    "\n",
    "#### Summary: Get the Data (eksklusiv Create the Workspace og Download the Data)\n",
    "\n",
    "- Check data, which features is included? Methods for doing so e.g(pandas) .head(), .info() og .describe() -  from the \n",
    "algoritm see $\\sigma$ and  $\\sigma^2$ as well as features's numerical sizes (counts) procentiles, min and max values\n",
    "    - The information above can be visualized using histograms(matplotlib). \n",
    "- Is any of the data already been preprocessed, in which case is this of importance to us?\n",
    "    - We might need to collect new data for existing samples\n",
    "    - Or removed the affected samples from the set, so the model doesn't get trained with a bias.\n",
    "    \n",
    "    - We might need to do this to our own data set, as many \"track\" is not categorized in terms of popularity yet.\n",
    "    \n",
    "- Split the data in train/tets set - ROT, 20% test. Why: to avoid _data Snooping Bias_ \" (That we detect patterns we are searching for and include this i our decision of   model choice - overfitting) Furthermore program specific method for spitting data, train_test_split().\n",
    "    - Be caureful with how the data is split, if the model is trained over multiple runs and the training set have been ramdomixed on index, the model will over time be trained on a larger part of the data. We want it to not have seen the test set before were ready for the final validation. - There is methods to prevent this outcome (hashing of identifier).\n",
    "\n",
    "    - \" _Stratified Sampling_ \" The data need to represent the group it is chosen from. Make sure the set include the same distributin as the orginal dataset. In the book the method .cut is used. Stratisfied sampling allows a test set that is less skewed than ramdom sampling, measured distribution compared to the orginal data set. Statisfied sampling can be performed by using the class StratisfiedShuffleSplit\n",
    "\n",
    "#### Summary: Discover and Visualize the Data to Gain Insights,\n",
    "\n",
    "- Visulization of the data - generate plots to help looking for relationships in the data. E.g in the book scatter plots are generated from distric coordinates. Remember, distric is a collection of people, for whoose house data exist, meaning high desity of districs in cities!\n",
    "     - Use colors  and transparency to further clearify both desity and values( book: transparency for density, color for house_val)\n",
    "     \n",
    "     \n",
    "- Correlation, as from statistics.Correlation in the example would be crossings where the signals is similiar to each other, or rather, there exists a relationship? Scatter matrix is good, equal to \"coorelation matrix\" e.g on the diagonal the is auto corr.\n",
    "    - Visualization kan help with bpth finding relationship in data, but also underlying relation we might want to filter out - like the ceiling for \"meadian_house_value\" in the book.\n",
    "- Lastly, attributes (features) can be combined in hopes of higher correlation. In the example from the book, there is a high correlation between the number of bed room pr room and the house value. DataFrames makes it easy to generate combined features(as seen in the book).\n",
    "\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "- Data cleaning and feature scaling - The data needs to be prepared so it can be \"fed\" to the chosen model. The moset algoritms have a hard time handling null-data and data with variance in numerical data.\n",
    "    - Data cleaning, 3 options\n",
    "        - Vi can remove samples that include null data\n",
    "        - Remove features from all samples\n",
    "        - Fill in missing value e.g. using mean values.\n",
    "       \n",
    "    - Feature Scaling: Normalization and standardization of data, standardization is less sensitive to outliers (Example: Mistakes in housing data: median income : 100 -> 1 values 0-15 -> 0.15 - using normalization)\n",
    "    \n",
    "    - Data categories (one-hot encoding): To include features consisting of different categories can be diffiulr since these has to converted to numerical valules and are depedent on the feature describes the distance between features to different degrees. -The solution is one-hot encoding to create binary features for each category.\n",
    "    \n",
    "    - Automate transformation (Transformation pipeline): Pipelines can help automate preprocessing of data, so this can be performed on new data set, new data i det current set and evaluate feature engineering choices.\n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "- Focus is on choosing the correct model, but how does one go about? Remember, the performance measure is our reference! Things to take into consideration when training and measuring the model: \n",
    "    - Overfitting\n",
    "    - Underfitting\n",
    "- The above can be hard to determine unless you are in an outer case(horrible or perfect performance measure of model). So, what can one do to help deciding on a good model? \n",
    "    - Cross validation. Again, uses the idea to split dataset into train/test. Doing this 10 times, providing performance meassure for each validation. \n",
    "        - Makes it easier to determine a poorly choosen model! \n",
    "- Lastly, computation time is to be considered as well. Do NOT discard results of models, as the dataset can be big, having to reperform validations and trainings can be time consuming! Time is money friend. \n",
    "\n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "- When a short list of fitting models is selected its time to optimize the hyper parameters. This too can be automated using algorithms to train and crossvalidate the model using all possible combinations of defined set of parameters.\n",
    "     - RandomizedGridSearchCV: This algoritm tries random hyper parameters and returns their performance. -Helping you narrow in on the best params for your selected model.\n",
    "     - GridSearchCV: This algoritm tries out all possible combinations of a set of hyperparameters you predefined.\n",
    "         - Better for a small set of combinaton options.\n",
    "         - ParamGrid: Dictionary of key-list pairs defining paramters and combination options\n",
    " \n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "- For a real ML application its not over when a good model is reached. It's when the model has to be launched and then maintained. Systems have to be created to:\n",
    "     - Monitor live performance regularly and trigger alerts if it drops. \n",
    "     - Montoring often also require human analysis of prediction samples\n",
    "     - Input data has to evaluated as model generally tend to rot over time.\n",
    "     - Retrain your model using fresh data, as the input data changes over time\n",
    "     \n",
    "#### Resume: Try It Out!.\n",
    "- This section is a summary of the entire chapter and a call to action for us to apply the techniques to our own dataset.\n",
    "\n",
    "    - In our case with the spotity data, were primarily focused on a classification task: a model that can predict a musical number's genre based on other feature, presumably we are going to use supervised learning.\n",
    "    - We already have a dataset from Kaggle  containing around 233.0000 songs,( ~ 10000 per 26 unique genres), each with 16 features. - This is our preliminary analysis, and we need to analyze further.\n",
    "    - We need to test different models and the create a pipeline that will test different hyper parameters. Then we might also need a transformation pipeline to clean the data, as the dataset is a bit skewed since it do contain a number of tracks not yet categorised according to populariy. - We probably should not use mean values to fill this out, but is our sample set big enough if we drop all the tracks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
