{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/itmal28/IKT-MAL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.path.append(\"../include/\")\n",
    "basePath = os.getcwd()\n",
    "basePath = os.path.split(basePath)[0]\n",
    "basePath = os.path.split(basePath)[0]\n",
    "sys.path.append(os.path.join(basePath, \"include\"))\n",
    "print(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "from libitmal import dataloaders_v3 as itmaldataloaders\n",
    "\n",
    "#%%\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "data_csv_path =  basePath + \"/Slutprojekt/DatafordelingPlot/spotifyDBData_preprocessed_minmaxscaled.csv\"\n",
    "data_csv_path_onehot = basePath + \"/Slutprojekt/DatafordelingPlot/spotifyDBData_preprocessed_minmaxscaled_onehotenc-all.csv\"\n",
    "data_csv_path_onehot_mode_tsig = basePath + \"/Slutprojekt/DatafordelingPlot/spotifyDBData_preprocessed_minmaxscaled_onehotenc-mode-tsig.csv\"\n",
    "data_csv_path_onehot_mode_tsig_stdscaled = basePath + \"/Slutprojekt/DatafordelingPlot/spotifyDBData_preprocessed_stdscaled_onehotenc-mode-tsig.csv\"\n",
    "\n",
    "def loadPreprocessed():\n",
    "    return pd.read_csv(data_csv_path, sep=',', header=0)\n",
    "\n",
    "def loadPreprocessedOnehotenc():\n",
    "    return pd.read_csv(data_csv_path_onehot, sep=',', header=0)\n",
    "\n",
    "def loadPreprocessedModeTsigOnehotenc():\n",
    "    return pd.read_csv(data_csv_path_onehot_mode_tsig, sep=',', header=0)\n",
    "\n",
    "def loadPreprocessedModeTsigOnehotencStdScaled():\n",
    "    return pd.read_csv(data_csv_path_onehot_mode_tsig_stdscaled, sep=',', header=0)\n",
    "\n",
    "all_genres = ['Alternative', 'Anime', 'Blues', 'Children’s Music', 'Classical', 'Comedy',\n",
    " 'Country', 'Dance', 'Electronic', 'Folk', 'Hip-Hop', 'Indie', 'Jazz', 'Movie',\n",
    " 'Opera', 'Pop', 'R&B', 'Rap', 'Reggae', 'Reggaeton', 'Rock', 'Ska', 'Soul',\n",
    " 'Soundtrack', 'World']\n",
    "\n",
    "regular_input_features = [\n",
    "    \"popularity\", \"acousticness\", \"danceability\",\n",
    "    \"energy\", \"instrumentalness\",  \"liveness\", \"loudness\", \"speechiness\",\n",
    "    \"tempo\", \"valence\"\n",
    "]\n",
    "\n",
    "onehot_input_features = [\n",
    "    \"mode_major\", \"mode_minor\",\n",
    "    \"time_signature_3-4\", \"time_signature_4-4\", \"time_signature_5-4\"\n",
    "]\n",
    "\n",
    "def getSpotifyBinarizedXY(remove_cols=None):\n",
    "    data_filtered = loadPreprocessed()\n",
    "    if (remove_cols is not None):\n",
    "        for col in remove_cols:\n",
    "            data_filtered = data_filtered[data_filtered.genre != col]\n",
    "    X_df = data_filtered[regular_input_features]\n",
    "    lb = LabelBinarizer()\n",
    "    X = np.array(X_df)\n",
    "    y = lb.fit_transform(np.array(data_filtered['genre']))\n",
    "\n",
    "    print(\"Binarized Label to the following classes\")\n",
    "    print(lb.classes_)\n",
    "    print(lb.classes_.shape[0])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def getSpotifyIntLabeledXY():\n",
    "    spotifyDBData = loadPreprocessed()\n",
    "    X = np.array(spotifyDBData[regular_input_features])\n",
    "\n",
    "    label_encoder = LabelEncoder() \n",
    "    y = label_encoder.fit_transform(spotifyDBData['genre']) \n",
    "\n",
    "    return X, y\n",
    "\n",
    "def getSpotifyOneHotEncXY():\n",
    "    spotifyDBData = loadPreprocessedModeTsigOnehotenc()\n",
    "    input_features = regular_input_features + onehot_input_features\n",
    "    X = np.array(spotifyDBData[input_features])\n",
    "\n",
    "    y = LabelEncoder().fit_transform(spotifyDBData['genre']) \n",
    "    return X, y\n",
    "\n",
    "def getSpotifyOneHotEncStdScaledXY():\n",
    "    spotifyDBData = loadPreprocessedModeTsigOnehotencStdScaled()\n",
    "    input_features = regular_input_features + onehot_input_features\n",
    "    X = np.array(spotifyDBData[input_features])\n",
    "\n",
    "    y = LabelEncoder().fit_transform(spotifyDBData['genre']) \n",
    "    return X, y\n",
    "\n",
    "#%%\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            r=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                t = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(r)>0:\n",
    "                    r += ','\n",
    "                r += f'{key}={t}{value}{t}'  \n",
    "            return r            \n",
    "        try:\n",
    "            p = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + p + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(fetchmode=False)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    elif mode=='spotify_binarized':\n",
    "        #remove_cols = ['Alternative', 'Anime', 'Blues', 'Children’s Music', 'Classical', 'Comedy',\n",
    "            # 'Country', 'Dance', 'Electronic', 'Folk', 'Hip-Hop', 'Indie', 'Jazz', 'Movie',\n",
    "            # 'Opera', 'Pop', 'R&B', 'Reggae', 'Reggaeton']\n",
    "        remove_cols = None\n",
    "        X, y = getSpotifyBinarizedXY(remove_cols)\n",
    "    elif mode=='spotify_intlabels':\n",
    "        X, y = getSpotifyIntLabeledXY()\n",
    "    elif mode=='spotify_onehotin_labelout':\n",
    "        X, y = getSpotifyOneHotEncXY()\n",
    "    elif mode=='spotify_onehotin_labelout_std':\n",
    "        X, y = getSpotifyOneHotEncStdScaledXY()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}'\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    # assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print('OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: spotify_onehotin_labelout..\n",
      "  org. data:  X.shape      =(222593;   15), y.shape      =(222593)\n",
      "  train data: X_train.shape=(155815;   15), y_train.shape=(155815)\n",
      "  test data:  X_test.shape =(66778;   15), y_test.shape =(66778)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%% -----------------------------------\n",
    "#   Neural Network\n",
    "# -------------------------------------\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('spotify_onehotin_labelout')\n",
    "\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary  = to_categorical(y_test)\n",
    "\n",
    "assert y_train_binary.ndim==2\n",
    "assert y_test_binary.ndim ==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 155815 samples, validate on 66778 samples\n",
      "Epoch 1/35\n",
      "Epoch 2/35\n",
      "Epoch 3/35\n",
      "Epoch 4/35\n",
      "Epoch 5/35\n",
      "Epoch 6/35\n",
      "Epoch 7/35\n",
      "Epoch 8/35\n",
      "Epoch 9/35\n",
      "Epoch 10/35\n",
      "Epoch 11/35\n",
      "Epoch 12/35\n",
      "Epoch 13/35\n",
      "Epoch 14/35\n",
      "Epoch 15/35\n",
      "Epoch 16/35\n",
      "Epoch 17/35\n",
      "Epoch 18/35\n",
      "Epoch 19/35\n",
      "Epoch 20/35\n",
      "Epoch 21/35\n",
      "Epoch 22/35\n",
      "Epoch 23/35\n",
      "Epoch 24/35\n",
      "Epoch 25/35\n",
      "Epoch 26/35\n",
      "Epoch 27/35\n",
      "Epoch 28/35\n",
      "Epoch 29/35\n",
      "Epoch 30/35\n",
      "Epoch 31/35\n",
      "Epoch 32/35\n",
      "Epoch 33/35\n",
      "Epoch 34/35\n",
      "Epoch 35/35\n",
      "OK, training time=105.9\n",
      "loss: 2.495954499939759\n",
      "accuracy: 0.29982928509924867\n",
      "f1_score: 0.27030451178279374\n",
      "precision: 0.42805646714854567\n",
      "recall: 0.1998861900648269\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y, y_pred,**kwargs):\n",
    "        true_positives = K.sum(K.round(K.clip(y * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y, y_pred, **kwargs):\n",
    "        true_positives = K.sum(K.round(K.clip(y * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y, y_pred, **kwargs):\n",
    "    precision = precision_m(y, y_pred)\n",
    "    recall = recall_m(y, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# # fit the model\n",
    "# history = model.fit(Xtrain, ytrain, validation_split=0.3, epochs=10, verbose=0)\n",
    "\n",
    "# # evaluate the model\n",
    "# loss, accuracy, f1_score, precision, recall = model.evaluate(Xtest, ytest, verbose=0)\n",
    "\n",
    "#%%\n",
    "\n",
    "# Build Keras model \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=15, units=20, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(units=25, activation=\"softmax\"))\n",
    "\n",
    "#optimizer = SGD(lr=0.1)\n",
    "optimizer = Adam(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "# Train\n",
    "VERBOSE     = 10\n",
    "EPOCHS      = 35\n",
    "\n",
    "start = time()\n",
    "history = model.fit(X_train, y_train_binary, \n",
    "    validation_data=(X_test, y_test_binary), \n",
    "    epochs=EPOCHS, verbose=VERBOSE)\n",
    "t = time()-start\n",
    "\n",
    "print(f\"OK, training time={t:0.1f}\")\n",
    "\n",
    "\n",
    "#%%\n",
    "# score = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "\n",
    "# print(f\"Training time: {t:0.1f} sec\")\n",
    "# print({score[0]}) # loss is score 0 by definition?\n",
    "# print({score[1]})\n",
    "# print(f\"All scores in history: {score}\")\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test_binary, verbose=10)\n",
    "print(\"loss: \" + str(loss))\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "print(\"f1_score: \" + str(f1_score))\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "# y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# print(classification_report(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/opt/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\", line 210, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\", line 152, in fit\n    history = self.model.fit(x, y, **fit_args)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1039, in fit\n    validation_steps=validation_steps)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n    outs = f(ins_batch)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2697, in __call__\n    if hasattr(get_session(), '_make_callable_from_options'):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 186, in get_session\n    _SESSION = tf.Session(config=config)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1551, in __init__\n    super(Session, self).__init__(target, graph, config=config)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 676, in __init__\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5f738e51c897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m# # summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "# Use scikit-learn to grid search the number of neurons\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# scoring = {'acc': make_scorer(accuracy_score), 'f1': make_scorer(f1_score),\n",
    "#                              'precision': make_scorer(precision_score),'recall' : make_scorer(recall_score)}\n",
    "# scoring = {'acc': make_scorer(accuracy_score), 'f1': f1_m,\n",
    "#                              'precision': precision_m,'recall' : recall_m}\n",
    "scoring = {'f1': make_scorer(f1_m), 'precision': make_scorer(precision_m),'recall' : make_scorer(recall_m)}\n",
    "\n",
    "def createModel(neurons=1):\n",
    "    # Build Keras model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=15, units=neurons, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "    model.add(Dense(units=25, activation=\"softmax\"))\n",
    "\n",
    "    #optimizer = SGD(lr=0.1)\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=createModel, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "grid.fit(X_train, y_train_binary)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# # means = grid_result.cv_results_['mean_test_score']\n",
    "# # stds = grid_result.cv_results_['std_test_score']\n",
    "# # params = grid_result.cv_results_['params']\n",
    "# # for mean, stdev, param in zip(means, stds, params):\n",
    "# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
