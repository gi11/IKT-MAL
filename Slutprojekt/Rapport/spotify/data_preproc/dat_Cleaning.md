## Cleaning the data

From the visualization, a few key points was seen with regards to the data: 

- The popularity feature contains a significant amount of samples with a rating of 0.
- The "A Capella" genre only contains 119 samples
- The duration feature is in a too high range, and contains significant outliers
- Both the genre, key, time signature and mode are all labeled traits

Since the group has no idea what causes a popularity rating of 0 - Is it a new song without any data yet? Is there a threshold of listens/checks that causes the popularity rating? Is it just unpopuler with no interrest? - It has been decided to drop the samples with this rating from the dataset. This does however cause some implications, as our models ability to predict unpopular songs will be severely hindered. Futhermore, dropping samples from the already somewhat small dataset is not desireable, as it also hinders the models ability to gather information and improve itself. 

The "A Capella" genre does not contain enough samples to provide valuable information about the genre itself, no model will be able to learn from 119 samples, and almost certainly not distinguish between other genres from it (outliers will have too much effect as no real distribution can be made). 

<!-- Lastly, before moving on, the "_duration_ms_" feature contained significant outliers, while the feature itself had a huge range. -->

The duration feature was difficult to tackle, as one could no simply scale it to a range of 0 - 1, since the outliers would cause too much of a skew in the range and thus, the feature would face truncation issues. From Figure \ref{durErr} it is seen that each feature has at least 1 significant outlier for their duration feature, while all being distributed around a somewhat similar mean value. 

![Showcasing the max value of outlier (yellow) of the duration feature pr genre](img/dur_mean_var_max.png){#durErr width=72%}

<!-- One possible solution the group though of would be to scale the unit, either to second and minutes as this would leviate the issue of the huge range found in the feature. This, however, did not help the outlier issue. -->
To overcome the outlier issue, a manual boundary could be set in place, which would discard the outliers, and remove them from the dataset. The boundary could be found from analysis of the distribution plots, but this solution would cause further reduction in the dataset. Therefore, since the correlation to popularity was already low, and the distribution mean and meadians across genres are so similar, the feature itself is removed instead of discarding samples.

For the labeled features, some form encoding must be applied, and it has been decided to use one hot encoding for this. This does not come without a price, as this effectively increases the amount of features used in the models, and the size of the dataset should be sufficient to still be able to learn properly. Looking at just the genre, we are looking to expand the dataset with an additional 24 features - this could be problematic. 

The 'mode'-feature expands to 2 features, and 'time signature' to 3. Originally there were 5 distinct values for 'time signature' found in the dataset, 0/4, 1/4, 3/4, 4/4 and 5/4. But entries with 0/4 and 1/4 were removed from the dataset, as they are arguably meaningless in context of the dimension they are supposed describe, and it is unclear how they should be interpreted. In addition, there were few entries with 0/4 or 1/4, futher suggesting that they are anomalous artifacts of the procedure they were generated by, and their removal seems justified.
The 'key'-feature would expand to an additonal 12, and there was not found any real correlation, thus the feature was decided to be dropped. The rest was encoded. 

__OBS__: The genre has only been one hot encoded for the regressor, as alot of scikit learns classifiers contains _labelBinarizer()_[^3] inside them - thus, they do not expect their y-values to encoded beforehand. In the case of them expecting it to,  _labelBinarizer()_ should be used for in order to satisfy the the models of scikit learn. 

<!-- Performing the above cleaning, the popularity across the genres can be plotted again to verify that the 0 rated samples has indeed been filtered out:  -->

[^3]:https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer
